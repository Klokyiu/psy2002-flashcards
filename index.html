<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>PSY2002 Final Exam Revision Guide - WITH ANSWERS</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            margin: 20px;
            line-height: 1.6;
            background-color: white;
            color: black;
        }
        .page {
            max-width: 8.5in;
            margin: 20px auto;
            padding: 40px;
            background: white;
            page-break-after: always;
            box-shadow: 0 0 10px rgba(0,0,0,0.1);
        }
        h1 {
            text-align: center;
            font-size: 18px;
            margin-bottom: 20px;
        }
        h2 {
            font-size: 14px;
            font-weight: bold;
            margin-top: 15px;
            margin-bottom: 10px;
            border-bottom: 1px solid #ccc;
            padding-bottom: 5px;
        }
        .question {
            margin: 10px 0 5px 0;
            font-weight: bold;
            font-size: 11px;
        }
        .answer {
            color: blue;
            margin-left: 20px;
            font-size: 10px;
            margin-bottom: 8px;
            line-height: 1.4;
        }
        hr {
            margin: 15px 0;
            border: none;
            border-top: 1px solid #ccc;
        }
        .page-number {
            text-align: center;
            margin-top: 20px;
            font-size: 9px;
            color: gray;
            border-top: 1px solid #eee;
            padding-top: 10px;
        }
    </style>
</head>
<body>

<!-- PAGE 1: Complex Design & Quasi-Experimental -->
<div class="page">
    <h1>PSY2002 Final Exam Revision Guide<br>WITH ANSWERS</h1>
    <hr>

    <h2>COMPLEX DESIGN</h2>

    <div class="question">What is a complex design?</div>
    <div class="answer">A factorial design where 2 or more independent variables are studied simultaneously in one experiment. Each IV can be studied with independent group design or repeated measures design.</div>

    <div class="question">What does 2 X 2 mean in complex (factorial design)?</div>
    <div class="answer">It means there are 2 independent variables, each with 2 levels. The number of conditions is determined by multiplying the levels: 2 × 2 = 4 total conditions.</div>

    <div class="question">What is an interaction effect? What is a main effect?</div>
    <div class="answer">Main Effect: The effect of each IV alone on the DV, collapsed across or averaged over the levels of the other IV. Interaction Effect: The effect of one IV differs depending on the level of a second IV. Non-parallel lines on a graph suggest interaction effects exist.</div>

    <div class="question">Know how to read and compute values representing the main effect and the interaction effect of a study using a complex design</div>
    <div class="answer">Main effect is calculated by taking the mean of each IV level across all levels of the other IV. Interaction effect is identified by comparing whether the pattern of one IV's effect changes depending on the level of the other IV. Visual representation using line graphs helps identify interactions—parallel lines = no interaction, non-parallel lines = interaction exists.</div>

    <div class="question">How can we know if the study based on a complex design shows an interaction effect?</div>
    <div class="answer">If non-parallel lines are observed when graphing the results, an interaction effect is present. The effect of one IV differs depending on the level of the other IV. Use F-tests in inferential statistics to determine if the interaction is statistically significant.</div>

    <div class="question">What are ceiling and floor effects?</div>
    <div class="answer">Ceiling Effect: Performance reaches a maximum level in one or more conditions, preventing measurement of further improvement. Floor Effect: Performance reaches a minimum level, preventing measurement of further decline. These limit the ability to detect treatment effects and make interaction effects uninterpretable.</div>

    <h2>QUASI-EXPERIMENTAL DESIGN</h2>

    <div class="question">What are the confounds that could threaten the internal validity a true experiment?</div>
    <div class="answer">Threats to internal validity include: (1) History - events other than treatment affect outcomes; (2) Maturation - natural changes over time; (3) Testing - familiarity with testing procedure; (4) Instrumentation - changes in measurement instruments; (5) Regression - extreme scores reverting to mean; (6) Subject Attrition - loss of participants; (7) Selection - groups differing in ways other than treatment; (8) Additive Effects with Selection - combination of selection with history, maturation, or instrumentation.</div>

    <div class="question">What are different designs of quasi-experimental studies?</div>
    <div class="answer">Main quasi-experimental designs include: (1) Non-equivalent Control Group Design - comparing treatment and comparison groups without random assignment; (2) Interrupted Time-Series Design - repeated observations before and after treatment; (3) Time-Series With Non-equivalent Control Group - combining time-series with non-equivalent comparison group.</div>

    <div class="question">What is a non-equivalent control group design?</div>
    <div class="answer">A design where treatment and comparison groups are compared using pretest and posttest measures, but groups are selected on bases other than random assignment. The comparison group is meant to control threats to internal validity due to history, maturation, testing, instrumentation, and regression. However, assumptions about equivalence between groups cannot be fully held.</div>

    <div class="question">What are the confounds that could threaten the internal validity a non-equivalent control group designed study?</div>
    <div class="answer">Specific threats include: (1) Selection-Maturation Effect - individuals in different groups change at different rates; (2) Selection-Instrumentation Effect - changes in measuring instruments more likely detected in one group; (3) Differential Statistical Regression - regression more likely in one group than another; (4) Expectancy Effects - observer bias if aware of study hypothesis; (5) Contamination - communication of information between groups; (6) Novelty Effects - special attention to treatment group causing Hawthorne effect.</div>

    <div class="question">What is a program evaluation about?</div>
    <div class="answer">Program evaluation comprises research methodology to evaluate: (1) Need Assessment - whether unmet needs exist for which an agency might provide service; (2) Process - observational methods assessing program implementation; (3) Outcome - whether the program achieved stated goals; (4) Efficiency - examining the cost of the program. It provides feedback regarding human service activity effectiveness.</div>

    <div class="page-number">Page 1</div>
</div>

<!-- PAGE 2: Single Case Research Design -->
<div class="page">
    <h2>SINGLE CASE RESEARCH DESIGN</h2>

    <div class="question">Identify and describe different methods of single case studies</div>
    <div class="answer">Case Study Method involves intensive description and analysis of a single individual using qualitative data (naturalistic observation, archival records, interviews, psychological tests). Clinical case studies have little control over extraneous variables, making it difficult to infer causation about variables influencing individual behavior—unlike single-case experimental designs.</div>

    <div class="question">What are benefits and potential limitations of case study methods?</div>
    <div class="answer">Benefits: (1) Rich source of information about people and behaviors; (2) Contribution to new ideas and hypotheses; (3) Development of new clinical techniques; (4) Study of rare phenomena; (5) Theoretical support. Limitations: (1) Extraneous variables prevent valid causal inferences; (2) Cannot eliminate sources of bias (observer bias, memory bias); (3) Problems of generalization and external validity; (4) Reliance on subjective impressions; (5) Replication needed to ensure external validity.</div>

    <div class="question">What is a single-case (small-N) experimental design?</div>
    <div class="answer">Experimental analysis of behavior with N=1 or small number of subjects. More rigorous than case studies, it involves manipulation of independent variables and continuous monitoring of behavior. Baseline behavior is recorded before treatment, then compared with behavior during treatment. Associated with Applied Behavior Analysis, behavior modification, and behavior therapy.</div>

    <div class="question">What is an ABAB design? Why is it called an ABAB design?</div>
    <div class="answer">ABAB design (Reversal design) involves: A (Baseline) - recording subject's behavior before treatment; B (Treatment) - introducing treatment while monitoring behavior; A (Withdrawal of Treatment) - removing treatment to see if behavior reverts; B (Another Treatment Stage) - reintroducing treatment. Called ABAB because alternating baseline and treatment phases allow researchers to infer whether behavior changes following introduction and withdrawal of treatment. This demonstrates that the treatment, not other variables, caused the behavior change.</div>

    <div class="question">What is the purpose of having multiple baselines in multiple-baseline design?</div>
    <div class="answer">Multiple baselines serve to strengthen causal inferences by illustrating that behavior in more than one baseline changes following the introduction of treatment. This design allows researchers to demonstrate that treatment effects are specific to when and where treatment is applied, rather than due to confounding variables or natural changes over time.</div>

    <div class="question">What are different ways of having multiple baselines in multiple-baseline design?</div>
    <div class="answer">Three types: (1) Within Individual Across Situations - treatment introduced in one situation while monitoring behavior in other situations; change appears only in treated situation. (2) Across Individuals - baselines first established for different individuals, then intervention introduced at different times for each individual; change occurs after treatment for each individual. (3) Within Individual Across Behaviors - treatment directed at one behavior, then another; performance changes for each behavior immediately after treatment introduction.</div>

    <div class="question">When there is a large baseline variability in single-case designs, what can we do?</div>
    <div class="answer">When baseline shows excessive variability before treatment, making it difficult to draw conclusions about treatment effects: (1) Examine and remove factors producing variability; (2) Continue taking baseline measures until behavioral measures stabilize before introducing treatment; (3) Identify variables that might be confounding with the treatment variable; (4) Replicate the procedure with different subjects.</div>

    <div class="question">What are the problems associated with all single case design?</div>
    <div class="answer">Common problems: (1) Excessive baseline variability makes it difficult to detect treatment effects; (2) Limited external validity—findings may not generalize beyond the individual(s) studied; (3) In ABAB designs, ethical issues arise when removing treatment; (4) In ABAB, behavior may not revert to baseline when treatment is withdrawn due to other variables taking control; (5) In multiple-baseline designs, behavior changes may appear in baseline before treatment is applied; (6) Opportunity for participants receiving late treatment to imitate behavior of early-treated participants.</div>

    <div class="page-number">Page 2</div>
</div>

<!-- PAGE 3: Survey Research Part 1 -->
<div class="page">
    <h2>SURVEY RESEARCH</h2>

    <div class="question">What is survey research?</div>
    <div class="answer">Survey research is designed to deal directly with the nature of people's thoughts, opinions, and feelings. It addresses theoretical questions and solves practical problems through: (1) Selection of representative samples; (2) Use of predetermined set of questions for all respondents; (3) Description of attitudes of the population from sample; (4) Comparison of attitudes between different populations; (5) Examination of changes in attitudes over time.</div>

    <div class="question">Describe all key terms in sampling</div>
    <div class="answer">Population: Set of all cases of interest. Sample: Subset of population drawn from sampling frame (specific list of members in population). Sampling Frame: The specific list of members in the population. Representative Sample: Sample that exhibits the same distribution of characteristics as the population. Biased Sample: Characteristics systematically different from population characteristics. Selection Bias: Procedures used to select sample result in over- or under-representation of some segment. Representativeness: Degree to which sample reflects true population characteristics.</div>

    <div class="question">What are the differences among simple random sampling, stratified random sampling, and convenient sampling?</div>
    <div class="answer">Simple Random Sampling: Every element has equal chance of inclusion; in homogeneous populations, small samples can be representative; in heterogeneous populations, larger samples needed. Stratified Random Sampling: Population divided into subpopulations (strata); random samples drawn from each stratum either equal-sized or proportional basis; more representative than simple random for heterogeneous populations. Convenient Sampling (Non-probability): Selection based on availability and willingness to respond; lacks representativeness; creates biased samples; typically unreliable for generalizing.</div>

    <div class="question">Understand how sample size is determined by on stratified random sampling</div>
    <div class="answer">Sample size in stratified random sampling can be determined two ways: (1) Equal-sized samples from each stratum - draw same number from each group regardless of population size (e.g., 30 students from each year); (2) Proportional basis - draw elements proportional to stratum size in population (e.g., if population is 40% Year 1, draw 40% of sample from Year 1). Proportional basis samples are more representative of the overall population.</div>

    <div class="question">What are advantages and disadvantages of different survey methods?</div>
    <div class="answer">Mail Surveys - Advantages: Quick completion, elimination of interviewer bias, suitable for personal topics with anonymity. Disadvantages: High cost, low response rate, little control over responses. Personal Interviews - Advantages: Greater flexibility, clarification possible, follow-up questions, high response rate. Disadvantages: High cost of training, interviewer bias. Telephone Interviews - Advantages: Representative sample via random-digit dialing, lower cost. Disadvantages: Selection bias, limited to phone users, low response rate for long surveys. Internet Surveys - Advantages: Low cost, large diverse samples. Disadvantages: Selection bias (internet users only), response bias, lack of environmental control.</div>

    <div class="question">What are the problems in longitudinal design of survey studies?</div>
    <div class="answer">Main problems: (1) Attrition - respondents drop out over time; sample size decreases; sample becomes less representative; characteristics of non-respondents should be compared with respondents remaining. (2) Consistent Responses Over Time - respondents strive to give consistent answers rather than change to avoid appearing inconsistent. (3) Reactive Measurement - initial survey may sensitize respondents to the issue; people behave differently because they know they're participating in study.</div>

    <div class="page-number">Page 3</div>
</div>

<!-- PAGE 4: Survey Research Part 2 & Data Analysis -->
<div class="page">
    <h2>SURVEY RESEARCH (Continued)</h2>

    <div class="question">What is the reliability and the test-retest reliability? How can we increase the reliability of a measure?</div>
    <div class="answer">Reliability: Consistency of measurement. Test-Retest Reliability: Administer same questionnaire to sample at two different time points; relative positions of scores between time points should be similar; high test-retest reliability indicated by correlation coefficient .80 or above. Ways to increase reliability: (1) Include more items in measure; (2) Increase variability on factor being measured; (3) Use testing situation free of distractions; (4) Provide clear instructions.</div>

    <div class="question">What is the validity? What are different types of validity?</div>
    <div class="answer">Validity: Truthfulness of a measure—the extent to which it measures what it claims to measure. Types: (1) Construct Validity - extent measure assesses theoretical construct it's designed to measure; includes Convergent Validity (similar constructs should correlate) and Discriminant Validity (dissimilar constructs should be distinct); (2) Content Validity - extent measure adequately samples relevant content domain; (3) Criterion Validity - extent measure predicts relevant outcomes.</div>

    <div class="question">What are the suggestions when writing your own questions in survey studies?</div>
    <div class="answer">Guidelines: (1) Use simple, direct, familiar vocabulary for all respondents; (2) Avoid double-barreled, leading, or loaded questions; (3) Use shorter sentences; (4) Present conditional information first, then key ideas; (5) Avoid response bias using extreme points or midpoints; (6) Use reverse-scored questions to prevent acquiescence bias; (7) Check readability; (8) Report exact wording with data. Questioning Order: For self-administered surveys—start with interesting questions, end with demographic questions. For personal/telephone interviews—begin with demographic questions as easy warm-up.</div>

    <div class="question">What are examples of bad survey questions?</div>
    <div class="answer">Bad questions include: (1) Double-barreled questions - asking two things in one question (e.g., "Do you think schools should have better facilities and equipment?"); (2) Leading questions - suggesting preferred answer (e.g., "Most people believe... do you agree?"); (3) Loaded questions - emotionally charged language; (4) Ambiguous questions - unclear wording; (5) Questions with vague response categories; (6) Questions too long or complex; (7) Questions using jargon unfamiliar to respondents.</div>

    <div class="question">Explain why a correlation is not a causation.</div>
    <div class="answer">Correlation cannot infer causal relationship because: (1) Direction cannot be determined - if outgoingness and life satisfaction are positively correlated, does outgoingness cause satisfaction or does satisfaction cause outgoingness? (2) Third variables can explain correlation (spurious relationships) - other unmeasured variables may explain the apparent relationship; (3) Mediator variables - other variables may explain the relationship mechanism; (4) Moderator variables - may affect direction or strength of relationship. Survey research is mostly correlational, limiting causal inferences.</div>

    <h2>BASICS IN DATA ANALYSIS AND INTERPRETATION</h2>

    <div class="question">Make sure you understand and know how to interpret all key statistical notation and terms covered in the lecture (e.g., mean, median, mode, standard deviations, outliers, different types of plots, correlation coefficient, effect size, formula to get Cohen's d, confidence interval, margin of error, standard error of the mean)</div>
    <div class="answer"><strong>Central Tendency:</strong> Mean: Average score. Median: Middle score. Mode: Most frequent score. <strong>Dispersion:</strong> Range: Highest-lowest. Standard Deviation (SD): Average distance from mean (calculated with n-1). Outliers: Extreme scores far from mean. <strong>Visual Displays:</strong> Histogram: Frequency distribution. Box-plot: Quartiles, median, outliers. Scatter plot: Relationship between two variables. <strong>Inferential Statistics:</strong> Standard Error of Mean (SEM): Estimates error in estimating population mean. Correlation Coefficient (r): Ranges -1 to +1. Effect Size (Cohen's d): Magnitude of treatment effect. Confidence Interval (CI): Range of values containing true population parameter. Margin of Error: Range around sample estimate.</div>

    <div class="question">Understand steps in data analysis</div>
    <div class="answer">Three stages: (1) Getting to Know the Data - check errors, missing data, invalid responses, outliers; create visual displays; examine distribution shape. (2) Summarizing the Data - compute descriptive statistics (mean, median, mode, SD); calculate correlation coefficients if correlational study; compute effect size. (3) Confirming What Data Reveal - use inferential statistics (t-tests, F-tests, confidence intervals) to determine whether IV produced reliable effect on DV; rule out whether findings due to chance error variation.</div>

    <div class="page-number">Page 4</div>
</div>

<!-- PAGE 5: Observational Design -->
<div class="page">
    <h2>OBSERVATIONAL DESIGN</h2>

    <div class="question">How do we identify representative samples of behaviors?</div>
    <div class="answer">Methods for identifying representative behavioral samples: (1) Time Sampling - choosing time intervals systematically or randomly for observations; combined systematic and random procedures. (2) Event Sampling - recording when infrequently occurring or unpredictable events happen (e.g., sports psychologists recording specific game behaviors). (3) Situation Sampling - observing behaviors in different locations, circumstances, and conditions. (4) Subject Sampling - selecting subjects systematically or randomly; ensure many behaviors can be effectively observed.</div>

    <div class="question">What are the differences among naturalistic observation, participant observation, structured observation, and field experiment?</div>
    <div class="answer">Naturalistic Observation: Passive recorder of events; establishes external validity; describes behaviors as they ordinarily occur. Participant Observation: Observer plays dual role—observes and participates actively; can be disguised or undisguised; raises ethical/privacy issues. Structured Observation: Observer intervenes to create or set up situations for easier observation; uses confederates; natural setting with systematic control. Field Experiment: Manipulation of one or more IVs in natural setting to determine effect on behavior; combination of experimental control and naturalistic setting.</div>

    <div class="question">What are indirect (unobtrusive) observational methods?</div>
    <div class="answer">Indirect observational methods study behavior without direct observation or participant awareness: (1) Physical Traces - remnants/fragments of past behavior; Use Traces (evidence of item use), Products (creations/artifacts). (2) Archival Records - public/private documents; Running Records (continuously kept/updated), Episodic Records (occasional entries), Personal Documents. These measures are valuable innovations for studying behaviors; truthfulness must be verified through independent sources.</div>

    <div class="question">What are the differences among physical and use traces?</div>
    <div class="answer">Physical Traces: Remnants or fragments of past behavior; classified as Natural Traces (observed without intervention, reflecting naturally occurring events) or Controlled-Use Traces (some researcher intervention). Use Traces: Physical evidence resulting from the use or nonuse of an item; examples include worn furniture indicating high traffic areas, or untouched items indicating disuse. Both provide unobtrusive evidence of behavior patterns.</div>

    <div class="question">What are the limitations of indirect (unobtrusive) observational methods?</div>
    <div class="answer">Limitations: (1) Selective Deposit - some information selected for archival deposit while other information is not; incomplete record. (2) Selective Survival - records may be missing or incomplete; surviving records may not represent original population. (3) Problems identifying spurious relationships - evidence may falsely indicate variables are associated. (4) Validity of physical-trace measures must be examined carefully and verified through independent sources. (5) Alternative hypotheses may explain observed physical traces.</div>

    <div class="question">What are the four types of measurement scales? What are differences among these measurement scales? Use examples to describe these differences</div>
    <div class="answer"><strong>Nominal:</strong> Categories with no order; classification; frequencies/percentages. Example: Gender, School. <strong>Ordinal:</strong> Natural ordering, unequal intervals; ranking data. Example: Educational level. Stats: Median, Mode. <strong>Interval:</strong> Equal intervals, no true zero. Example: Temperature (°C), IQ scores. Stats: Mean, SD, Pearson r. <strong>Ratio:</strong> Equal intervals with true zero point; highest level. Example: Age, Height, Weight. All statistics appropriate; most powerful for analysis.</div>

    <div class="page-number">Page 5</div>
</div>

<!-- PAGE 6: Observational Design Continued & Exam Tips -->
<div class="page">
    <h2>OBSERVATIONAL DESIGN (Continued)</h2>

    <div class="question">What are the steps in qualitative data analysis?</div>
    <div class="answer">Steps: (1) Comprehensive Records - after observation, write descriptions using narrative records, audio/video; researchers classify and organize records to test hypotheses. (2) Field Notes - observer's running descriptions of participants, events, settings, behaviors of interest; requires accurate content and precision; trained observers essential. (3) Coding/Units of Analysis - identify relevant sources helping answer research questions; sample selections from source; code behavioral units. (4) Data Reduction - abstract and summarize data; verbally summarize information, identify themes, categorize pieces of information. (5) Content Analysis - objective coding technique; identify descriptive categories and themes based on study objectives.</div>

    <div class="question">What is an observer reliability?</div>
    <div class="answer">Observer Reliability: Extent to which recorded observations can be verified by independent observers and through other means of investigation. Also called Inter-observer Reliability. Calculation depends on measurement scale: For Nominal Scale—independent observers unaware of other's recordings; when events classified by mutually exclusive categories, reliability ≥ 85% considered acceptable. For Ordinal Scale—Spearman rank-order correlation used. For Interval/Ratio Scale—Pearson Product-Moment Correlation Coefficient used.</div>

    <div class="question">What is the reactivity in observational studies? How can we reduce the reactivity in observational studies?</div>
    <div class="answer">Reactivity: Presence of observer leads people to change their normal behavior; threatens external validity. Related to Demand Characteristics—research participants guess what behaviors expected, behaving unnaturally. Ways to reduce reactivity: (1) Disguised Participant Observation - participants unaware they're observed; raises ethical concerns. (2) Naturalistic Observation - passive observation in natural settings without intervention. (3) Use of Unobtrusive Methods - hidden cameras, one-way mirrors, physical traces, archival records. (4) Habituation - observer enters setting repeatedly until participants stop reacting. (5) Limit Participant Knowledge - restrict information about observer's role. (6) Indirect Observation - use unobtrusive methods instead of direct observation.</div>

    <div class="question">What are the sources of bias in observational studies? How can we reduce the observational bias?</div>
    <div class="answer"><strong>Sources of Observer Bias:</strong> (1) Expectancy Effects - when observers aware of study hypotheses, expectations lead to systematic errors. (2) Observer Expectations - determine which behaviors to observe; expectations cause systematic errors. (3) Selective Attention - researchers choose what to observe; biases what's recorded. (4) Interpretation Bias - subjective interpretation varies by observer background. <strong>Ways to reduce bias:</strong> (1) Awareness of Own Biases. (2) Use of Automated Equipment - reduces subjective interpretation. (3) Limit Information to Observers - restrict knowledge about study objectives. (4) Clear Operational Definitions. (5) Multiple Independent Observers - verify reliability. (6) Standardized Procedures - use same procedures each time. (7) Selection of Recording Angle, Location, Time - objective decisions.</div>

    <hr>
    <h2 style="text-align: center; margin-top: 20px; border: none;">GENERAL EXAM TIPS:</h2>
    <div style="margin-left: 20px; font-size: 11px; line-height: 1.6;">
    ✓ Be prepared to analyze factorial designs with computed main and interaction effects<br>
    ✓ Understand threats to internal and external validity across all design types<br>
    ✓ Know when correlation ≠ causation and be able to explain why<br>
    ✓ Memorize key statistical terms and when they're appropriately used<br>
    ✓ Be ready to differentiate between observational methods and their appropriate uses<br>
    ✓ Attempt all questions in your textbook!<br><br>
    <strong>✓ Final Exam: Wed, 10-Dec-2025, 1400-1600 HKT</strong><br>
    <strong>✓ 50 MCQ + 2 Short Questions</strong><br>
    ✓ Closed book exam<br>
    ✓ No consultation after 1400 HKT on 8-Dec-2025
    </div>

    <hr style="margin-top: 20px;">
    <p style="text-align: center; font-size: 9px; color: gray; margin-top: 15px;">
    Generated from PSY2002 Lecture Notes: Complex Design, Quasi-Experimental Design, Single-Case Design, Survey Research, Data Analysis, and Observational Design
    </p>

    <div class="page-number">Page 6</div>
</div>

</body>
</html>
